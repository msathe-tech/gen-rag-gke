# Brag Your RAG With the MLOPS Swag
# Brag Your RAG With Kubernetes Swag
# Barg Your RAG With GKE Swag

Organizations are beginning to unlock significant value by integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) into their business-critical processes. However, enterprises often face challenges in meeting the high expectations of GenAI-driven business outcomes. Bridging this gap requires meticulous planning in governance, continuous evaluation, seamless scaling, operational costs, and time-to-market.

In this session, we will share evolving patterns and practices that enterprises can adopt using open standards. Attendees will witness a live demonstration of a RAG application stack built with LangChain, Canopy, and a PostgreSQL Vector database, all deployed on Kubernetes. Additionally, we will discuss leveraging GPU and TPU accelerators to enhance computational efficiency. The audience will also gain insights into MLOps strategies for data splitting, embeddings, retrieval, and prompt engineering. Join us to explore how to effectively leverage MLOps with Kuberetes to achieve scalable and impactful GenAI solutions.

https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/cfp/


jiten.kumar@gmail.com
## Scatchpad 

We will demonstrate supercharging the applications using accelerators such as GPUs and TPUs. 

We will leverage Kuberetes with accelerators such as GPUs & TPUs. 


[7:05 PM] Jitender Kumar
During the learning process, emphasis is on automation streamlines repetitive tasks such as data preprocessing, model training, and deployment, leading to faster and more efficient workflows. Focus will be on  Devops/automation and MLOps to facilitate the seamless scaling of AI solution for teams to reduce operational costs, and accelerate time-to-market for data-driven innovations.
 

[7:00 PM] Jitender Kumar
Also this  can be used to automate the creation of high-quality content, such as reports, articles, and marketing materials, saving time and resources.
 

By synthesizing data from multiple sources, RAG and LLMs can provide comprehensive analyses and recommendations, aiding in more informed and strategic decision-making processes.

[6:54 PM] Jitender Kumar
Together, RAG and LLMs can streamline workflows by automating routine tasks and information synthesis, thereby increasing productivity and efficiency. They enhance customer service through precise and personalized query handling, leading to improved customer satisfaction and loyalty. Additionally, these technologies support advanced data analytics, providing actionable insights that can drive innovation and competitive advantage. Overall, RAG and LLMs offer transformative potential for organizations, enabling smarter, faster, and more informed business operations.

[6:53 PM] Jitender Kumar
Retrieval-Augmented Generation (RAG) combined with Large Language Models (LLMs) can significantly enhance real-world organizational efficiency by providing accurate, context-aware responses using vast data repositories. RAG enables dynamic access to up-to-date, relevant information, reducing time spent on manual research. LLMs improve decision-making by generating coherent and insightful outputs from complex data. Together, they streamline workflows, enhance customer service through precise query handling, and support advanced data analytics for strategic insights.
 


We will use PostgreSQL as a OSS Vector DB and deploy it on GKE for scalability. 
We will use Gemini's embedings API to build vector embedings and then store in the PostgreSQL. 
Follow the docs here - https://cloud.google.com/kubernetes-engine/docs/tutorials/deploy-pgvector. 
The PostgreSQL offers compatibility with managed services on GCP. 
We will use the Langchain deployed on GKE with  to build a solution to chunk the PDFs and extract text. 

We will build a DevOps pipeline to deploy the code and build the GKE based platform using Terraform. 



